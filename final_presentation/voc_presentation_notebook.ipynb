{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FileUtil Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.file_util import FileUtil\n",
    "\n",
    "file_util = FileUtil()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Raw Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = file_util.get_raw_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.transformations import apply_cleaning_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = apply_cleaning_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.sentiment_analysis.train.train import sentiment_analysis_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.topic_modelling.train.train import topic_modelling_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modelling_train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = file_util.get_metrics(\"sentiment_analysis\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_prauc = sorted(list(map(lambda item: (item[0], item[1][\"PR AUC\"]), metrics.items())), key = lambda x: x[1])\n",
    "print(\"Best model is {} with PR-AUC {}\".format(models_prauc[-1][0], models_prauc[-1][1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = file_util.get_topics_html(\"LDA\")\n",
    "fig.update_layout(width = 700, height = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = file_util.get_topics_html(\"BERTopic\")\n",
    "fig.update_layout(width = 700, height = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = file_util.get_topics_html(\"NMF\")\n",
    "fig.update_layout(width = 700, height = 550)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict reviews_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.predict import predict_sentiment_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_util.TEST_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_util.best_sentiment_analysis_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bert = predict_sentiment_topic()\n",
    "test_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_bert.drop([\"cleaned_text\", \"subtopic\", \"topic\"], axis = 1)\n",
    "test_output = test_output.rename(columns = {\"partially_cleaned_text\": \"Text\", \"date\": \"Time\", \"sentiment\": \"predicted_sentiment\", \n",
    "                            \"sentiment_prob\": \"predicted_sentiment_prob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output.to_csv(\"final_presentation/reviews_test_predictions_h2o2.ai.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra (Prediction with Other Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yml\") as f:\n",
    "     config_params = yaml.safe_load(f)\n",
    "\n",
    "config_params[\"best_sentiment_analysis_model\"] = \"Logistic Regression\"\n",
    "\n",
    "with open(\"config.yml\", \"w\") as f:\n",
    "    yaml.dump(config_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_util.best_sentiment_analysis_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logreg = predict_sentiment_topic()\n",
    "test_logreg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\") as f:\n",
    "     config_params = yaml.safe_load(f)\n",
    "\n",
    "config_params[\"best_sentiment_analysis_model\"] = \"LSTM\"\n",
    "\n",
    "with open(\"config.yml\", \"w\") as f:\n",
    "    yaml.dump(config_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_util.best_sentiment_analysis_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm = predict_sentiment_topic()\n",
    "test_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\") as f:\n",
    "     config_params = yaml.safe_load(f)\n",
    "\n",
    "config_params[\"best_sentiment_analysis_model\"] = \"BERT\"\n",
    "\n",
    "with open(\"config.yml\", \"w\") as f:\n",
    "    yaml.dump(config_params, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.visualisation.dashboard_viz import *\n",
    "\n",
    "vis_df = reformat_data(test_bert)\n",
    "pio.renderers.default = \"svg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations for sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pie_chart_fig = sentiment_pie_chart(vis_df)\n",
    "sentiment_trend_fig = sentiment_line_chart_over_time(vis_df)\n",
    "topics_sentiment_fig = topics_bar_chart(vis_df)\n",
    "\n",
    "display(sentiment_pie_chart_fig.update_layout(width = 500, height = 300, title='Overall Sentiment Breakdown'))\n",
    "display(sentiment_trend_fig.update_layout(title='Sentiment trend'))\n",
    "display(topics_sentiment_fig.update_layout(title='Topics by Sentiment'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations for topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_pie_chart_fig = topics_pie_chart(vis_df)\n",
    "topics_bar_chart_fig = topics_bar_chart_over_time(vis_df, time_frame='Q')\n",
    "top_key_words_fig = visualise_all_topics(vis_df)\n",
    "\n",
    "display(topics_pie_chart_fig.update_layout(width = 500, height = 300, title='Frequency of topics'))\n",
    "display(topics_bar_chart_fig.update_layout(title='Topics over Time'))\n",
    "display(top_key_words_fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations for specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtopics in each topic\n",
    "select_topic = 'Drinks'\n",
    "\n",
    "subtopic_fig = get_subtopics(vis_df, topic=select_topic)\n",
    "subtopic_sentiment_fig = sentiment_pie_chart(vis_df[vis_df[\"topic\"]==select_topic])\n",
    "\n",
    "display(subtopic_sentiment_fig.update_layout(width = 500, height = 300,  title=f'Sentiment Breakdown for {select_topic}'))\n",
    "display(subtopic_fig.update_layout(width = 500, height = 300))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.unittest.unit_testing\n",
    "from src.unittest.unit_testing import unit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Methods in unit testing:\", [method for method in dir(src.unittest.unit_testing) if method[:4] == \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        if os.path.basename(root) == \"__pycache__\":\n",
    "            continue\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files(\"src\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.file_util import FileUtil\n",
    "print(\"Methods in FileUtil:\", [func for func in dir(FileUtil) if callable(getattr(FileUtil, func)) and not func.startswith(\"__\")])\n",
    "print(\"Attributes in FileUtil:\", list(FileUtil().__dict__.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.sentiment_analysis.train.bert import BERT\n",
    "print(\"Methods in BERT:\", [func for func in dir(BERT) if callable(getattr(BERT, func)) and not func.startswith(\"__\")])\n",
    "print(\"Attributes in BERT:\", list(BERT().__dict__.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.topic_modelling.train.lda import LDA\n",
    "print(\"Methods in LDA:\", [func for func in dir(LDA) if callable(getattr(LDA, func)) and not func.startswith(\"__\")])\n",
    "print(\"Attributes in LDA:\", list(LDA().__dict__.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.topic_modelling.train.bertopic import BERTopic_Module\n",
    "print(\"Methods in BERTopic_Module:\", [func for func in dir(BERTopic_Module) if callable(getattr(BERTopic_Module, func)) and not func.startswith(\"__\")])\n",
    "print(\"Attributes in BERTopic_Module:\", list(BERTopic_Module().__dict__.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docstrings Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(FileUtil.put_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment_topic.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sentiment_analysis_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(topic_modelling_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
